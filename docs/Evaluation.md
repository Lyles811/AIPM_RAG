# 评估与回归测试（从 v0.1 到更严谨）

## v0.1（当前实现）
- 规则评测：must_include 命中率（非常粗略但可回归）
- 命令：
```bash
python eval_run.py
# 输出：data/eval/report.md
```

## 为什么要评测？
AI 产品上线后最怕：
- 你改了 prompt/检索参数，感觉更好，但其实某些场景变差了
- 成本/时延突然飙升而你没意识到
评测让你能用数据做决策（更像真实 AI PM 工作）。

## 下一步怎么升级（建议你作为迭代亮点写进简历）
1) 结构化输出评测（强烈建议）
- 要求回答输出 JSON（结论/步骤/引用）
- 用 JSON Schema 校验结构合规率

2) 引用正确性（Groundedness）
- 检查答案关键句是否能在引用片段中找到依据
- 或人工抽样评审并记录标签

3) 失败原因分类
- 答非所问 / 引用缺失 / 引用不匹配 / 覆盖不足 / 语言质量差
- 做一个失败样例库（Top 20）

4) A/B 对比（版本决策）
- prompt v1 vs v2
- top_k=3 vs 5
- embedding 模型 A vs B
输出一份“选型报告”（非常像公司流程）
