# PRD：AI 产品经理基础知识库问答（RAG）

- 版本：v0.1（MVP）
- 日期：2026-01-13
- 项目类型：开源作品集（可复现、可上线 Demo）

---

## 1. 背景与问题
### 1.1 背景
“AI 产品经理”相关内容在网络上很多，但常见问题是：
- 知识碎片化：概念多、框架多、口径不统一；
- 缺少系统化的“从 0→1 落地路径”：PRD/MVP/评估/上线/迭代怎么做；
- 缺少“可查证”的回答：没有引用来源时，答案可信度低。

### 1.2 目标用户
- **转岗/入门**：想从研发/算法转 AI 产品经理的同学；
- **新人 PM**：需要系统复盘产品基本功 + AI 特殊性；
- **自学者**：希望快速搭建结构化知识体系。

---

## 2. 产品目标
### 2.1 北极星目标（North Star）
帮助用户在“AI 产品经理基本功”上 **更快获得可执行、可引用的答案**，减少检索与整理成本。

### 2.2 成功标准（高层）
- 用户能在 1 分钟内得到“结论 + 解释 + 可执行清单 + 引用来源”；
- 当知识库不覆盖时，系统明确提示“覆盖不足”，避免胡编。

---

## 3. MVP 范围（必须闭环）
### 3.1 Must-Have（v0.1）
1. 知识库载入：读取 `data/kb/` 下 Markdown 文件  
2. 索引构建：切分 chunk → embedding → 向量索引  
3. 问答：输入问题 → 检索 → 生成回答（固定结构）  
4. 引用：回答必须附 `chunk_id` 引用（可展开查看原文片段）  
5. 反馈：👍/👎 + 原因，记录到 `data/feedback/feedback.csv`  
6. 离线评测：基于 `data/eval/golden.jsonl` 一键回归评测，输出 report

### 3.2 Nice-to-Have（v0.2）
- 更严格的引用正确性校验（quote match / answer-grounding）
- rerank（重排）提升检索质量
- 支持 PDF/网页导入（目前先用 Markdown）

### 3.3 Won’t（本项目不做）
- 账号体系、多租户、权限管理
- 商业化收费/支付
- 高并发与 SLA（作品集 Demo 范畴）

---

## 4. 关键用户故事（User Stories）
- 作为一个想转岗 AI PM 的人，我希望输入“PRD 应该怎么写”，得到结构化回答并能看到引用来源。
- 作为一个新人 PM，我希望学习“如何拆 MVP”，系统能给出可执行清单。
- 作为知识库维护者，我希望新增文档后能重新构建索引并立即可用。
- 作为项目作者，我希望有离线评测，能够证明每次迭代“变好/变差”。

---

## 5. 验收口径（Definition of Done）
> 注：MVP 阶段口径要 **可测、可复现**，不追求完美但必须能回归。

### 5.1 质量（Quality）
- 引用覆盖率：答案中必须至少出现 1 条引用来源
- 覆盖不足时：必须明确提示“知识库未覆盖/信息不足”，并说明需要补充什么

### 5.2 体验（UX）
- 输出结构固定：结论 / 解释 / 可执行清单 / 引用来源

### 5.3 成本与性能（Cost & Latency）
- 记录单次问答耗时（p50/p95 作为迭代参考）
- 记录 token / 费用（可后续增强）

### 5.4 可迭代（Iteration）
- 反馈记录可用（CSV）
- 离线评测可一键运行并生成报告（`data/eval/report.md`）

---

## 6. 风险与对策
- 幻觉/编造：强制“只根据片段回答”，不足则拒答；输出必须带引用
- Prompt Injection：不执行知识库中的“指令”，只视为内容；回答依据来源片段
- 成本失控：限制 top-k、限制上下文片段数量、建议设置访问限流（线上）
- 版权风险：知识库优先使用自写笔记/公开授权内容；避免直接搬运付费文章

---

## 7. 里程碑
- v0.1：Markdown 知识库 + 索引 + RAG + 引用 + 反馈 + 离线评测
- v0.2：rerank + 引用正确性评测 + 失败原因分类看板
